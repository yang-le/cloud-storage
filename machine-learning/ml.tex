\documentclass[hyperref, UTF8]{ctexart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algorithmic}

\DeclareMathOperator{\tr}{tr}

\begin{document}

\title{机器学习笔记}
\author{
Le Yang\\
yangle0125@qq.com
}
\date{}
\maketitle

\tableofcontents

\part{监督学习}
记号说明：
我们将使用$x^{(i)}$表示“输入”变量,也叫做\emph{特征};使用$y^{(i)}$表示“输出”或者说\emph{目标}变量。
$(x^{(i)}, y^{(i)})$称为一个\emph{训练样本}, 而用来训练的数据集,$\lbrace(x^{(i)}, y^{(i)}); i = 1, \dots, m\rbrace$称为\emph{训练集合}。
注意这里的$(i)$仅仅作为索引使用，$m$是训练集合中训练样本的数量。我们也将使用$\mathcal{X}$表示输入变量所在的空间，用$\mathcal{Y}$表示输出变量所在的空间。

为了更正式一点地描述监督学习的问题，我们的目标是，给定一个训练集合，尝试学习一个函数$h:\mathcal{X} \mapsto \mathcal{Y}$,使得$h(x)$相对于对应的$y$来说是一个“好”的预测。因为某些历史上的原因，这个函数$h$被称为一个\emph{hypothesis（假设）}。

当要预测的目标变量是连续的时，称这个学习问题是一个\emph{回归}问题;当要预测的目标只取少量的离散值时，称这个学习问题是一个\emph{分类}问题。

\section{线性回归}
作为初始尝试，让我们假定$y$是$x$的线性函数：
\begin{equation*}
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2
\end{equation*}
这里，这些$\theta_i$被称为\emph{参数}(或称\emph{权重})。在不会引起误解的情况下，我们可以扔掉$h_\theta(x)$的下标$\theta$，将其简单地写作$h(x)$。
为了简化记号，我们再引入一个约定，令$x_0 = 1$(即\emph{截距}项)，所以：
\begin{equation}
h(x) = \sum_{i = 0}^n\theta_ix_i = \theta^Tx
\end{equation}
在最右侧，请将$\theta$和$x$都看作向量;这里$n$是输入变量的个数(不包括$x_0$)。现在，给定一个训练集合，我们应该如何选择，或者说学习，参数$\theta$呢？
一个合理的方法是，选择这样的$\theta$，使得$h(x)$尽量接近$y$，至少在训练集合内应该这样。为了说地更正式一些，我们将定义一个函数用来衡量对于每一个$\theta$的值$h(x^{(i)})$和对应的$y^{(i)}$到底有多接近。我们定义\emph{损失函数}:
\begin{equation}
J(\theta) = \frac{1}{2}\sum^m_{i = 1}(h_\theta(x^{(i)}) - y^{(i)})^2
\end{equation}

\subsection{LMS(最小均方)算法}
我们要选择$\theta$来最小化$J(\theta)$。为了这样做，让我们使用一种搜索算法，其始于对$\theta$的某些“初始猜测”，然后重复地改变$\theta$来让$J(\theta)$越来越小。对于这个问题，让我们考虑\emph{梯度下降}算法。它基于某些初始的$\theta$值，重复地执行如下的更新(这个更新是对所有的$j = 0,\dots,n$同时进行的)：
\begin{equation*}
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)
\end{equation*}

这里$\alpha$被称为\emph{学习速率}。这是一个非常自然的算法，重复地向着$J(\theta)$减少最大的方向更新$\theta$。为了实现这个算法，我们需要求出最右边一项的偏导数。让我们先假设训练集合中只有一个训练样本$(x, y)$，这样我们就不用考虑$J$的定义中的求和符号了，我们有：
\begin{equation}
\begin{split}
\frac{\partial}{\partial\theta_j}J(\theta) &= \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x) - y)^2 \\
&= 2\cdot\frac{1}{2}(h_\theta(x) - y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x) - y) \\
&= (h_\theta(x) - y)\cdot\frac{\partial}{\partial\theta_j}(\sum_{i = 0}^n\theta_ix_i - y) \\
&= (h_\theta(x) - y)x_j
\end{split}
\end{equation}
对于单个训练样本，这就给出了更新规则：\footnote{我们使用记号“$a := b$”表示一个(计算机程序中的)操作，含义是我们将\emph{设置}一个变量$a$的值，使其等于$b$的值。换句话说，这个操作将使用$b$的值来覆盖$a$。相反的，我们使用记号"$a = b$"来断言一个事实，即$a$的值和$b$的值是相等的。}
\begin{equation*}
\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}_j
\end{equation*}

这个规则被称为\emph{LMS}更新规则(LMS代表``least mean squares''),在某些地方，也被称为\emph{Widrow-Hoff}学习规则。
这个规则有一些特点使其看起来很自然，例如，更新的幅度是正比于\emph{误差}项$(y^{(i)} - h_\theta(x^{(i)}))$的。
因此，如果我们遇到一个样本，我们的预测和实际的值很接近，那么按照这个规则，$\theta$就几乎不需要做什么更新；反之，如果我们的预测和实际值相差很远，参数就会有较大幅度的更新。

我们在只有一个样本的情况下推导出了LMS规则。有两种方法来修改这一规则使其适用于多于一个样本的训练集合，第一种方法是使用下面的算法：
\begin{algorithm}[H]
\caption{批量梯度下降法(BGD)}  
\begin{algorithmic}    
\REPEAT  
\STATE $\theta_j := \theta_j + \alpha\sum_{i = 1}^m(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}_j$ (for every $j$)
\UNTIL{Convergence} 
\end{algorithmic}  
\end{algorithm}
该算法在做每一步更新时，都需要查看所有的数据，因此被称为\emph{batch gradient descent（批量梯度下降）}。还有另外一种算法工作地也很好:
\begin{algorithm}[H]
\caption{随机梯度下降法(SGD)}  
\begin{algorithmic}    
\REPEAT  
\STATE $\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}_j$ (for every $j$)
\STATE $i := (i + 1) \mod m$
\UNTIL{Convergence} 
\end{algorithmic}  
\end{algorithm}
该算法在做每一步更新时，只需查看训练集合中的下一个样本即可。被称为\emph{stochastic gradient descent（随机梯度下降）}或者是\emph{incremental gradient descent（增量梯度下降）}。

\subsection{正则方程}
梯度下降是一种最小化$J(\theta)$的方法，现在让我们来讨论另外一种。这次我们将直接求解$J(\theta)$的最小值，而不需要求助于迭代算法。在这种方法中，我们将求得$J(\theta)$相对于$\theta_j$的导数，令其得零。首先让我们来引入一些矩阵求导的记号。

\subsubsection{矩阵求导}
对于一个将$m \times n$的矩阵映射到实数的函数$f:\mathbb R^{m \times n} \mapsto \mathbb R$，我们定义函数$f$对矩阵$A$的导数为:
\begin{equation}
\nabla_Af(A) = 
\begin{bmatrix}
\frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f}{\partial A_{m1}} & \cdots & \frac{\partial f}{\partial A_{mn}}
\end{bmatrix}
\end{equation}
因此，$\nabla_Af(A)$本身也是个$m \times n$的矩阵，它的第$(i, j)$元素是$\partial f/\partial A_{ij}$。
我们还要引入“迹”的概念，写做``$\tr$''。对一个$n \times n$的(方)矩阵$A$，$A$的迹被定义为其对角线上元素的和：
\begin{equation}
\tr A = \sum_{i = 1}^nA_{ii}
\end{equation}
如果$a$是一个实数(也就是，一个$1 \times 1$的矩阵)，那么$\tr a = a$。迹的定义导致其有这样的属性，即对于两个矩阵$A$和$B$，如果$AB$是方阵，那么$\tr AB = \tr BA$。因此，我们还将有，比如说：
\begin{gather*}
\tr ABC = \tr CAB = \tr BCA \\
\tr ABCD = \tr DABC = \tr CDAB = \tr BCDA
\end{gather*}
迹的如下属性也很容易验证，这里$A$和$B$都是方阵，$a$是一个实数
\begin{align}
\tr A &= \tr A^T \\
\tr (A + B) &= \tr A + \tr B \\
\tr aA &= a\tr A
\end{align}
我们不加证明地
\footnote{就证明了，怎么地吧。

\eqref{eq1}$(\partial /\partial A_{ij})\tr AB = (\partial /\partial A_{ij})\sum_i\sum_jA_{ij}B_{ji} = B_{ji}$

\eqref{eq2}按照定义，$(\nabla_{A^T}f(A))_{ij} = (\partial /\partial A^T_{ij})f(A) = (\partial /\partial A_{ji})f(A)$

\eqref{eq3}$(\partial /\partial A)\tr ABA^TC \\= (\partial /\partial A)\sum_{w,x,y,z}A_{wx}B_{xy}A_{zy}C_{zw} \\= (\partial /\partial A_{wx})\sum_{w,x,y,z}A_{wx}B_{xy}A_{zy}C_{zw} + (\partial /\partial A_{zy})\sum_{w,x,y,z}A_{wx}B_{xy}A_{zy}C_{zw} \\= B_{xy}A_{zy}C_{zw} + A_{wx}B_{xy}C_{zw} = (BA^TC)_{xw} + (CAB)_{zy} = (C^TAB^T)_{wx} + (CAB)_{zy}$。

\eqref{eq4}定义$A$关于第$i$列第$j$行的余子式（记作$M_{ij}$）是去掉$A$的第$i$行第$j$列之后得到的$(n − 1)\times(n − 1)$矩阵的行列式。$A$关于第$i$列第$j$行的代数余子式是：$A'_{ij} = (-1)^{i+j}M_{ij}$。$A$的余子矩阵是一个$n \times n$的矩阵$A'$，使得其第$i$行第$j$列的元素是$A$关于第$i$行第$j$列的代数余子式。矩阵$A$的伴随矩阵是$A$的余子矩阵的转置矩阵$A'^T$。作为拉普拉斯公式的推论，关于$n \times n$矩阵$A$的行列式,有$AA'^T = |A|I$。这表明$A' = |A|(A^{-1})^T$。注意到$|A| = \sum_jA_{ij}A'_{ij}$。因为$A'_{ij}$与$A_{ij}$无关(这可以从定义看出来)，这就意味着$(\partial/\partial A_{ij})|A| = A'_{ij}$，而这就是所要证的。}
给出矩阵求导的如下属性，\eqref{eq4}只适用于非奇异的方阵$A$，其中$|A|$代表$A$的行列式。我们有：
\begin{align}
\nabla_A\tr AB &= B^T  \label{eq1} \\
\nabla_{A^T}f(A) &= (\nabla_Af(A))^T  \label{eq2} \\
\nabla_A\tr ABA^TC &= CAB + C^TAB^T  \label{eq3} \\
\nabla_A|A| &= |A|(A^{-1})^T \label{eq4}
\end{align}

\subsubsection{最小二乘}
让我们继续求解$J(\theta)$的最小值，首先我们需要将$J$写成矩阵-向量记号的形式。给定一个训练集合，定义\emph{设计矩阵}$X$为一个$m \times n$的矩阵(实际上是$m \times (n + 1)$，如果包括截距项的话)。该矩阵的每一行都是一个输入变量：
\begin{equation*}
X =
\begin{bmatrix}
\text{---} (x^{(1)})^T \text{---} \\
\text{---} (x^{(2)})^T \text{---} \\
\vdots \\
\text{---} (x^{(m)})^T \text{---}
\end{bmatrix}
\end{equation*}
再定义$\vec{y}$为一个$m$维的向量，包含训练集合中的所有目标变量：
\begin{equation*}
\vec{y} =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
\end{equation*}
现在，因为$h_\theta(x^{(i)}) = (x^{(i)})^T\theta$，很容易验证：
\begin{equation*}
\begin{split}
X\theta - \vec{y} &=
\begin{bmatrix}
(x^{(1)})^T\theta \\
\vdots \\
(x^{(m)})^T\theta
\end{bmatrix}
-
\begin{bmatrix}
y^{(1)} \\
\vdots \\
y^{(m)}
\end{bmatrix} \\
&=
\begin{bmatrix}
h_\theta(x^{(1)}) - y^{(1)} \\
\vdots \\
h_\theta(x^{(m)}) - y^{(m)} 
\end{bmatrix}
\end{split}
\end{equation*}
注意到对于向量$z$，我们有$z^Tz = \sum_iz^2_i$，利用这一事实，就得到：
\begin{equation*}
\begin{split}
\frac{1}{2}(X\theta - \vec{y})^T(X\theta - \vec{y}) &= \frac{1}{2}\sum_{i = 1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 \\
&= J(\theta)
\end{split}
\end{equation*}
最后，为求得$J$的最小值，让我们对$J$求导：
\begin{equation*}
\begin{split}
\nabla_\theta J(\theta) &= \nabla_\theta\frac{1}{2}(X\theta - \vec{y})^T(X\theta - \vec{y}) \\
&= \frac{1}{2}\nabla_\theta(\theta^TX^TX\theta - \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
&= \frac{1}{2}\nabla_\theta\tr(\theta^TX^TX\theta - \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
&= \frac{1}{2}\nabla_\theta(\tr\theta^TX^TX\theta - 2\tr\vec{y}^TX\theta) \\
&= \frac{1}{2}(X^TX\theta + X^TX\theta - 2X^T\vec{y}) \\
&= X^TX\theta - X^T\vec{y}
\end{split}
\end{equation*}
令该导数为零，我们就得到了\emph{正则方程}：
\begin{equation}
X^TX\theta = X^T\vec{y}
\end{equation}
因此，使得$J(\theta)$最小的$\theta$就是：
\begin{equation}
\theta = (X^TX)^{-1}X^T\vec{y}
\end{equation}

\subsection{概率解释}

\end{document}

